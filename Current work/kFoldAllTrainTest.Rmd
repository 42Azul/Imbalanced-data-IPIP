---
title: "IPIP Examples and different configurations"
author: "Fernando de la Gandara Fernandez"
editor_options:
  markdown:
    wrap: 72
output: pdf_document
---

```{r}
library(foreach)
library(doParallel)
library(DataExplorer)
library(dplyr)
library(pROC)
library(caret)
library(parallel)
library(DMwR)
library(ggplot2)
library(GGally)
library(corrplot)


NUM_CORES = detectCores()-1
options(mc.cores = NUM_CORES)

sink("./Outputs/CurrentOutput.txt")

#Set random seed
set.seed(42)
```

First of all we obtain the needed data and do the preprocessing to the
necessary elements as well as set up the output variable, and train/test differences.

```{r}
data <- read.csv("../Datasets/data_tfg.csv")
ind.cualit <- c(which(names(data) == "SITUACION"),which(names(data)=="SEXO"),which(names(data)=="DM"):which(names(data)=="DC"))
OUTPUT_VAR = "SITUACION"


# For correlation plot of some characteristics
# minoritary <- data[data$SITUACION == "FALLECIDO", ]
# majoritary <- data[data$SITUACION == "CURADO", ]
# M<-cor(minoritary[, !(1:ncol(minoritary) %in% ind.cualit)])
# corrplot(M, method = "circle")
# 
# M<-cor(majoritary[, !(1:ncol(majoritary) %in% ind.cualit)])
# corrplot(M, method = "circle")



 for(i in ind.cualit){
  
  data[,i] <- as.factor(data[, i])
  

 }
```

We look at imbalanced data in a table

```{r}
barplot(prop.table(table(data[[OUTPUT_VAR]])),
        col = rainbow(2),
        ylim = c(0, 1.01),
        main = "Class Distribution")
```
We now make the train/test stratified split

```{r}
train.index <- createDataPartition(data[[OUTPUT_VAR]], p = 0.85, list = FALSE)
data.train <- data[ train.index,]
data.test  <- data[-train.index,]

```
And the number of elements per class in the train set

```{r}

output_lev <- levels(data.train[[OUTPUT_VAR]])

lev_nrow <- c(nrow(data.train[data.train[[OUTPUT_VAR]] == output_lev[1],]), nrow(data.train[data.train[[OUTPUT_VAR]] == output_lev[2],]))

if (lev_nrow[1] < lev_nrow[2]){
  OUTPUT_MIN = output_lev[1]
  OUTPUT_MAJ = output_lev[2]
  nmin = lev_nrow[1]
  nmax = lev_nrow[2]
}else{
  OUTPUT_MIN = output_lev[2]
  OUTPUT_MAJ = output_lev[1]
  nmin = lev_nrow[2]
  nmaj = lev_nrow[1]
}
print(sprintf("%s : %d", OUTPUT_MIN, nmin))
print(sprintf("%s : %d", OUTPUT_MAJ, nmaj))

```




We define the metrics to be measured, amongst which the kappa factor or balanced accuracy will be
significants value to measure imbalanced data and will be the
differential measure during the training. One of them will be used in
the training and the other for measuring purposes

```{r}


kappa_mine <- function(TP, TN, FP, FN, lObs) {
    po <- (TP + TN) / lObs
    pe <- ((TP + FP) * (TP + FN) + (FP + TN) * (FN + TN)) / lObs^2
    (po - pe) / (1 - pe)
  }

sensitivity_mine <- function(TP, TN, FP, FN) {
    TP / (TP + FN)
}

specificity_mine <- function(TP, TN, FP, FN) {
    TN / (TN + FP)
  }

# metrics <- function(data, lev = levels(as.factor(data$obs)), model = NULL) {
#   #Calculate TP, TN, FP, FN
#   pred = data[,"pred"]
#   obs = data[,"obs"]
#   
#   TP <- sum(pred == OUTPUT_MIN & obs == OUTPUT_MIN)
#   TN <- sum(pred == OUTPUT_MAJ & obs == OUTPUT_MAJ)
#   FP <- sum(pred == OUTPUT_MIN & obs == OUTPUT_MAJ)
#   FN <- sum(pred == OUTPUT_MAJ & obs == OUTPUT_MIN)
#   
#   # Calculate Cohen's kappa and BalAcc
#   KAPPA <- kappa_mine(TP, TN, FP, FN, length(obs))
#   BAL_ACC <- (sensitivity_mine(TP, TN, FP, FN) + specificity_mine(TP, TN, FP, FN)) / 2
#   
#   # Return metrics as a named vector
#   c(KAPPA = KAPPA, BAL_ACC = BAL_ACC)
# }

metrics <- function(data, lev = levels(as.factor(data$obs)), model = NULL){
    sensi = sensitivity(data[, "pred"],data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ)
    met <- c(
    KAPPA = psych::cohen.kappa(cbind(data[, "obs"],data[, "pred"]))$kappa,
    SENS = sensi,
    BAL_ACC = ( sensi + specificity(data[, "pred"], data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ))/2
    )

  return(met)
}



metrics_all <- function(data, lev = levels(as.factor(data$obs)), model = NULL){
    met <- c(
    ACCURACY = MLmetrics::Accuracy(data[, "pred"], data[, "obs"]),
    SENS = sensitivity(data[, "pred"],data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ),
    SPEC = specificity(data[, "pred"], data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ),
    PPV = posPredValue(data[, "pred"], data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ),
    NPV = negPredValue(data[, "pred"], data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ),
    KAPPA = psych::cohen.kappa(cbind(data[, "obs"],data[, "pred"]))$kappa,
    BAL_ACC = (sensitivity(data[, "pred"],data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ) + specificity(data[, "pred"], data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ))/2,
    F1 = MLmetrics::F1_Score(data[,"pred"],data[,"obs"]),
    AUC = MLmetrics::AUC(data[,"prob"],data[,"obs.prob"]),
    PR = MLmetrics::PRAUC(data[,"prob"],data[,"obs.prob"]),
    MCC = mltools::mcc(data[,"pred"],data[,"obs"]),
    GEOM = sqrt(sensitivity(data[, "pred"],data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ)*specificity(data[, "pred"], data[, "obs"],positive=OUTPUT_MIN,negative=OUTPUT_MAJ))
  )
  return(met)
}

number_metrics = 12
#We add time as a measure we will compute later
metric_names <- c("ACCURACY", "SENS", "SPEC", "PPV", "NPV", "KAPPA", "BAL_ACC", "F1", "AUC", "PR", "MCC", "GEOM", "TIME")
```

Let us remember the different parameters of our model:

-   np: Elements of the minoritary class in each partition
-   p: Number of partitions to be made
-   b: Maximum number of models in each ensemble of each partition
-   mt: Function which returns how many trials we must make in the
    greedy sequencial approach

In order to stablish these parameters we will create a function for each
one of them:

First of all the one related to the "np" parameter which in our case
will be reduced to a function which gets the 75% of the minoritary
class.

```{r}
calculate_np <- function( nmin, nmaj,  elbow_prob=0.75) 
{
  np <- round(nmin*elbow_prob)
  return(np)
}

```

Now we move on to the number of partitions which, in our case, is
obtained using the formula studied in the original work, based on
applying the logarithmic division of values less than 1 (both negative)
to obtain a ratio between the established alpha and the number of
elements chosen per partition.

```{r}
calculate_p <- function( np, prob_codo=0.75, alpha_p= .01) 
{
  p <- ceiling(log(alpha_p)/(log(1-1/np)*np))
  return(p)
}

```

Subsequently, we approach the parameter b, which is responsible for
determining how many models at most there will be in each ensemble of
each partition made. To do this, we will take a similar approach to that
of the number of partitions, except that we will use a relationship
between nmin and np, and not just the number of minority elements per
partition (p). This way, b is always above the number of partitions
made.

```{r}

  
calculate_b <- function(np, nmin, nmaj, elbow_prob=0.75, alpha_b= .01) 
{

  b <- ceiling(log(alpha_b)/(log(1-1/nmin)*np))
  return(b)
}



```

And lastly, we will define the "mt" function, which calculates how many
tries at most should be made in order to enlarge each model ensemble in
the sequencial approach. It can be shown that the "mt" number of tries
will always be smaller than the "b" which are the maximum models for
each ensemble.

```{r}



mt <- function(b, n) { ceiling((b-n) / 3) }


```

This is an auxiliary function that just returns a continuous apply of a
given configuration "b" times. Useful for multiple seed algorithm
trainings (i.e five GBM models)

```{r}
get_function_vector <- function(b,function_training){
  function_vector <- c() 
  for(i in 1:b){
    function_vector <- append(function_vector, function_training) 
  } 
  

  return(function_vector)
}

```

Now let us set up some seed algorithms for the model. This function list
will be made by four well known ones:

-   Ranger
-   Logistic regression
-   SVM
-   GBM

The idea is to train them in different approaches and see which IPIP
variant works better, so we will make a k-fold of each of the next
approaches and see which of them seem to be a better idea.

-   Each seed algorithm alone (without IPIP)
-   Sequential IPIP with many instances of the same seed algorithm
-   Exhaustive IPIP with different seed algorithms (one of each at
    least)
-   Exhaustive IPIP but only the best ensemble

```{r}


seed_algorithms <- c(

#RFOREST
 function(df.train, metrics, OUTPUT, metric_optimize) {

    
    tC <- trainControl(
      summaryFunction = metrics,
      allowParallel = TRUE,
      method = "cv",
      number = 5,
      classProbs = TRUE
    )
    
    
    method <- "ranger"
    maximize <- T
    
    cl <- makeCluster(NUM_CORES, type="FORK" )
    clusterExport(cl, c("OUTPUT_MIN", "OUTPUT_MAJ"))
    registerDoParallel(cl)
    
    
    rf <- train(
      as.formula(sprintf("%s ~.", OUTPUT)),
      data = df.train,
      method = method,
      metric = metric_optimize,
      maximize = maximize,
      importance = "impurity",
      trControl = tC
    )
    
    stopCluster(cl)
    registerDoSEQ()
    
    return(rf)
  },
#RLOG
 function(df.train, metrics, OUTPUT, metric_optimize) {


    tC <- trainControl(
      summaryFunction = metrics,
      allowParallel = TRUE,
      classProbs = TRUE
    )
    
    method <- "glmnet"
    maximize <- T
    
       
    cl <- makeCluster(NUM_CORES, type="FORK")
    clusterExport(cl, c("OUTPUT_MIN", "OUTPUT_MAJ"))
    registerDoParallel(cl)
    
    
    rlog <- train(as.formula(sprintf("%s ~.", OUTPUT)),
      data = df.train,
      method = "glmnet",
      family = 'binomial',
      metric = metric_optimize,
      maximize = T,
      tuneGrid = expand.grid(
        alpha = 0:1,
      lambda = seq(0.0001, 1, length = 100)
      ),
      trControl = tC
    )
    
        
    stopCluster(cl)
    registerDoSEQ()

    
    return(rlog)
  },
#SVM
function(df.train, metrics, OUTPUT, metric_optimize) {

    tC <- trainControl(
      summaryFunction = metrics,
      allowParallel = TRUE,
      classProbs = TRUE
    )
    
    method <- "svmLinear"
    maximize <- T

    
        
        
    cl <- makeCluster(NUM_CORES, type="FORK")
    clusterExport(cl, c("OUTPUT_MIN", "OUTPUT_MAJ"))
    registerDoParallel(cl)
    
    
    svm <- train(
      as.formula(sprintf("%s ~.", OUTPUT, metric_optimize)),
      data = df.train,
      method = method,
      metric = metric_optimize,
      maximize = maximize,
      trControl = tC
    )
    
    stopCluster(cl)
    registerDoSEQ()
    
    return(svm)
  },
#GBM
 function(df.train, metrics, OUTPUT, metric_optimize) {

    tC <- trainControl(
      summaryFunction = metrics,
      allowParallel = TRUE,
      classProbs = TRUE,
      verboseIter = FALSE
      
    )
    
    method <- "gbm"
    maximize <- T
    
            
    cl <- makeCluster(NUM_CORES, type="FORK")
    clusterExport(cl, c("OUTPUT_MIN", "OUTPUT_MAJ"))
    registerDoParallel(cl)
    
    gbm <- train(as.formula(sprintf("%s ~.", OUTPUT)),
                 data = df.train,
                 method = method,
                 metric = metric_optimize,
                 maximize = maximize,
                 verbose = FALSE,
                 trControl = tC
    )
    
    stopCluster(cl)
    registerDoSEQ()
    
    return(gbm)
  })


alg_names <-c("RANGER", "RLOG", "SVM", "GBM")

```

Now that we have the configurations, we proceed to obtain a separation
into balanced subsets. To do this, I obtain a series of folds with the
created function 'imbalancedFold' that allow applying that division so
that each subset of the k-fold has the imbalanced subsets distributed

```{r}
imbalancedFold <- function(data, n_folds, target, minority_class) {
  n_samples <- nrow(data)
  n_majority_total <- n_samples - sum(data[[target]] == minority_class)
  n_minority_total <- sum(data[[target]] == minority_class)
  
  n_minority_per_fold <- ceiling(n_minority_total / n_folds)
  n_majority_per_fold <- ceiling(n_samples / n_folds - n_minority_per_fold)

  fold_indices <- list()
  
  for (i in 1:n_folds) {
    
    #We choose the samples of the majoritary class
    majority_indices <- which(data[[target]] != minority_class)
    used_majority_indices <- unlist(fold_indices)
    available_majority_indices <- setdiff(majority_indices, used_majority_indices)
    
    n_available_majority <- length(available_majority_indices)
    n_majority_this_fold <- min(n_majority_per_fold, n_available_majority)
    selected_majority_indices <- sample(available_majority_indices, size = n_majority_this_fold)
    
    #We take the samples of the minoritary class
    
    minority_indices <- which(data[[target]] == minority_class)
    used_minority_indices <- setdiff(used_majority_indices, available_majority_indices)
    available_minority_indices <- setdiff(minority_indices, used_minority_indices)
    n_available_minority <- length(available_minority_indices)
    n_minority_this_fold <- min(n_minority_per_fold, n_available_minority)
    selected_minority_indices <- sample(available_minority_indices, size = n_minority_this_fold)
    
    #We collect and combine both indexes
    selected_indices <- c(selected_majority_indices, selected_minority_indices)

    fold_indices[[i]] <- selected_indices
  }
  
  return(fold_indices)
}

```

In this case we will make a k-fold of k=5

```{r}

folds <-imbalancedFold(data.train, 5,OUTPUT_VAR, OUTPUT_MIN)

```

Now the prediction function is missing. In our case it consists of two
functions.

The first one is applied to the ensemble of models of a specific
partition. On each established model, the specific prediction (predict
method of the same) is made. Then a table is made that assigns to each
example the proportion of models that have predicted in that way. If
more than q (75% by default, that is, at least three-quarters of the
models predict that it is cured) have been predicted, it is established
that it is predicted as OUTPUT_MAJ and if not, it will be deceased.

Finally, the same thing is done but instead of using the predict
function of each model, the given prediction function on each ensemble
is used, the proportion is made on the ensemble sets and if the
prediction proportion value is \>50%, it is assumed as OUTPUT_MAJ being
if not OUTPUT_MIN.

```{r}

default_q = 0.75

prediction <- function(conj.model, x, q = default_q){ 
  pred <- data.frame(matrix(nrow=nrow(x),ncol=0))
  for(model in conj.model) pred <- cbind(pred, predict(model,x))
  nElems = ncol(pred)
  nElems <- ncol(pred)
  counts <- rowSums(pred == OUTPUT_MAJ)
  pred <- counts / nElems
  ifelse(is.na(pred) | pred<q, OUTPUT_MIN, OUTPUT_MAJ)
}

prediction.final <- function(ensemble, x, q = default_q){
  pred <- as.data.frame(lapply(ensemble, function(e) prediction(e,x)))
  nElems = ncol(pred)
  nElems <- ncol(pred)
  counts <- rowSums(pred == OUTPUT_MAJ)
  pred <- counts / nElems
  ifelse(is.na(pred) | pred<q, OUTPUT_MIN, OUTPUT_MAJ)
}

prediction.add <- function(old_ensemble, new_element, prediction_current = FALSE, test.set = FALSE){
    if(prediction_current == FALSE){
      prediction_current = as.numeric(unlist(predict(new_element, test.set)) == OUTPUT_MAJ)
    }
    if(length(old_ensemble$ensemble)!=0){
      prev_count =  old_ensemble$model_count
      l <- list("ensemble"= c(old_ensemble$ensemble,list(new_element)), "model_count"= prev_count + prediction_current)
      return(l)
    }
    else{
      l <- list("ensemble"= list(new_element), "model_count"=prediction_current)
      return(l)
    }
    
  
}

prediction.predicted <- function(predicted, q = default_q) {

    nElems <- length(predicted$ensemble) 
    pred <- ifelse(predicted$model_count  < q*nElems, OUTPUT_MIN, OUTPUT_MAJ)
    return(pred)
}


```
 
 
 Now let us add some functions which calculate the prediction probability needed for some metrics

```{r}
prediction.prob <- function(ensemble, x, q = default_q){
  pred <- data.frame(matrix(nrow=nrow(x),ncol=0))
  for(model in ensemble) pred <- cbind(pred, predict(model,x,type="prob")[[OUTPUT_MAJ]])
  pred<- rowSums(pred)
  pred
}

prediction.final.prob <- function(ensemble, x, q =default_q){
   #Place all predictions for a sample in each row of a data set
  pred <- as.data.frame(lapply(ensemble, function(e) prediction.prob(e,x)))
  pred <- rowSums(pred)/sum(unlist(lapply(ensemble,length)))
  pred
}



prop.maj = 0.55
metric_optimize = 'BAL_ACC'

```

Now we define the function that performs the training. It follows the
ideas explained in the work. The process consists of creating p
partitions where we ensure in each one a quantity of np elements of the
minority class, and then training multiple models on these elements.

To do this, we have a list called "dfs" that contains the indices of
each well-balanced partition. We will perform iterative training on each
of these partitions:

-   First, the model is trained on the first function of the function
    vector, and its metric results are stored.

-   Then, we try to expand the model until the "mt" attempt function
    limits it. To expand the model, it is trained with the next training
    function given by the parameter of functions, and the value obtained
    with that new function is compared with the value obtained without
    it. If an improvement is achieved, it is incorporated into the
    ensemble; otherwise, we move on to a new attempt.

-   Once the number of attempts for the current ensemble is complete, it
    is considered finished and is incorporated as the final model given
    for the partition.

We will continue this process for the p partitions, and we will have the
model as our final response.

```{r}
train_IPIP <- function( prop.maj, OUTPUT, min_str, maj_str, train.set, test.set, 
                        configuration, prediction, metrics, b, np,  p, mt, TRAIN_METRIC = "KAPPA", SELECT_METRIC = "SENS"){
  
  nmin = sum(train.set[[OUTPUT]] == min_str)
  nmaj = sum(train.set[[OUTPUT]] == maj_str)
  
  

  majSubSize <-  round(np*prop.maj/(1-prop.maj))
  minSubSize <-  np

  #Incluye en cada posicion los valores de los elementos de dicha particion, de 1 a p
  dfs <- list()
  
  minoritary <- subset(train.set, train.set[[OUTPUT]] == min_str)
  majoritary<- subset(train.set, train.set[[OUTPUT]] == maj_str)
  
  for(k in 1:p){
    id.minoritary <- sample(x = 1:nmin, size = minSubSize) #Index for minoritary class for each subset
    id.majoritary <- sample(x= 1:nmaj, size = majSubSize) #Indexes for majoritary class for each subset

    dfs[[k]] <- rbind(minoritary[id.minoritary,],majoritary[id.majoritary,])
  }
  
  
  E <- list() #Final model (ensemble of ensembles)


  for(k in 1:p){
    Ek <- list() # k-esim ensemble
    i <- 0 #Counter for number of tries of enlarging the ensemble
    metric.ensemble  = 0 #Variable for storing the metric of each ensemble accumulated.
    #Balanced partition
    
    df <- dfs[[k]]
    model_i = 1
    
    #We select the elements for the training
    majoritary <- which(df[[OUTPUT]] == maj_str)
    minoritary <- which(df[[OUTPUT]] == min_str)
    ind.train <- c(
        sample(majoritary, size = majSubSize, replace = TRUE),
        sample(minoritary, size = minSubSize, replace = TRUE) 
    )
    

    while(length(Ek$ensemble)<=b && i<mt(b,length(Ek$ensemble))){

    
      #We train with the models in configuration with a sequential approach

      model <- configuration[[length(Ek$ensemble)+1]](df[ind.train,], metrics, OUTPUT, TRAIN_METRIC)

      
      if (length(Ek$ensemble)==0){
          u <- -Inf;
          names(u) <- SELECT_METRIC;
          metrics.ensemble <- u
      }
      
      new_Ek <- prediction.add(Ek, model, test.set = test.set[colnames(test.set)!=OUTPUT])
      pred = prediction.predicted(new_Ek)
      metrics.ensemble.new <- metrics(data.frame(
        obs = test.set[[OUTPUT]],
        pred= as.factor(pred)
      ))
      #We check if the new model changes the result. If it does not we start trying again
      if(metrics.ensemble.new[SELECT_METRIC] <= metrics.ensemble[SELECT_METRIC]){ 
        i <- i+1
      } else{ 
        #If the ensemble tries to enlarge again, we restart the enlarging posibilities.
        Ek <- new_Ek
        metrics.ensemble <- metrics.ensemble.new
        i <- 0
      }
    } # End of the k-esim ensemble building

    E[[length(E)+1]] <- Ek$ensemble
  
  }
return(E);
}


```

Now we have another possible approach of IPIP based on an exhaustive
approach of the models.

```{r}
powerset = function(s, test.set){
    len = length(s)
    l = c()
    vector(length=2^len);l[[1]]=numeric()
    counter = 1L
    for(x in 1L:len){
        prediction_current =as.numeric(unlist(predict(s[x], test.set)) == OUTPUT_MAJ)
        prev_count = 0
        for(subset in 1L:counter){
            counter=counter+1
            if(subset == 1){
              l[[counter]] = list("ensemble"= s[x], "model_count"= prediction_current)
            }
            else{
              prev_count = l[[subset]]$model_count
              l[[counter]] = list("ensemble"= c(l[[subset]]$ensemble,s[x]), "model_count"= prev_count + prediction_current)
            }
        }
    }
    return(tail(l, length(l)-1))
}


train_IPIP_exhaustive <- function( prop.maj, OUTPUT, min_str, maj_str, train.set, test.set, configuration, prediction, metrics, np,  p, TRAIN_METRIC = "KAPPA", SELECT_METRIC = "SENS"){


  
  nmin = sum(train.set[[OUTPUT]] == min_str)
  nmaj = sum(train.set[[OUTPUT]] == maj_str)
  
  
  
  majSubSize <-  round(np*prop.maj/(1-prop.maj))
  minSubSize <-  np

  #Incluye en cada posicion los valores de los elementos de dicha particion, de 1 a p
  dfs <- list()
  
  minoritary <- subset(train.set, train.set[[OUTPUT]] == min_str)
  majoritary <- subset(train.set, train.set[[OUTPUT]] == maj_str)

  for(k in 1:p){
    id.minoritary <- sample(x = 1:nmin, size = minSubSize) #Index for minoritary class for each subset
    id.majoritary <- sample(x= 1:nmaj, size = majSubSize) #Indexes for majoritary class for each subset

    dfs[[k]] <- rbind(minoritary[id.minoritary,],majoritary[id.majoritary,])
  }
  

  
  E <- list() #Final model (ensemble of ensembles)


  for(k in 1:p){
      
    cat(sprintf("Ensemble number %d of %d\n", k,p))
    Ek <- list() # k-esim ensemble
    init = Sys.time()
  
    #Balanced partition
    
    df <- dfs[[k]]
    majoritary <- which(df[[OUTPUT]] == maj_str)
    minoritary <- which(df[[OUTPUT]] == min_str)
    ind.train <- c(
          sample(majoritary, size = majSubSize, replace = TRUE),
          sample(minoritary, size = minSubSize, replace = TRUE) 
    )
    train.test <- df[ind.train,]
      Ek <- list()
      for(conf in 1:length(configuration)){

        Ek[[length(Ek)+1]] <- configuration[[conf]](train.test, metrics, OUTPUT, TRAIN_METRIC)

      }
      
      all_sets <- powerset(Ek, as.data.frame(test.set[, -which(names(test.set) == OUTPUT)]))
      
      ##If we want to force bigger sets before smaller ones
      #lengths <- sapply(all_sets, function(x) length(x))
      # Reorder the list based on the lengths
      # all_sets <- all_sets[order(lengths, decreasing = TRUE)]
      

      
      max_metric <- function(model){
        pred = prediction.predicted(model)
        met <- metrics(data.frame(
              obs = test.set[[OUTPUT]],
              pred = as.factor(pred)
          ))[SELECT_METRIC]
          return(met)
      }
      
      
      
      #Run the max metric check in parallel mode
      max_metric_list <- lapply(all_sets, max_metric)
      #print(cbind(lapply(all_sets, function(x) length(x)/2), max_metric_list))
      # find the maximum chosen metric value and corresponding set
      
      max_metric_value <- max(unlist(max_metric_list))
      max_set <- all_sets[[which.max(unlist(max_metric_list))]]$ensemble
      cat(sprintf("Max metric %s of ensemble is %f with length %d\n", SELECT_METRIC, max_metric_value, length(max_set)))
      E[[length(E)+1]] <- max_set
  
  }
return(E);
}


```


```{r}
metricPreparedFrame <- function(model, setToTest)
      data.frame(
          obs = setToTest[[OUTPUT_VAR]],
          pred = predict(model,setToTest[,names(setToTest) != OUTPUT_VAR]),
          prob= prediction.final.prob(list(list(model)), setToTest[,names(setToTest) != OUTPUT_VAR]),
          obs.prob = as.numeric(ifelse(setToTest[[OUTPUT_VAR]] == OUTPUT_MAJ, 1, 0))
        )

```



Now we move on to performing the training for each configuration
established in the configuration list. To do this, we iterate through
each configuration and apply k-fold cross-validation. For each fold, we
provide a vector function associated with the maximum number of models b
for an ensemble. This approach allows for the creation of vectors of
variable functions


First we will make a naive approach over the base data using the seed algorithms

```{r warning=FALSE, cache=TRUE}
mean_metrics.seed <- list()
mean_metrics.test.seed <- list()
mean_time.seed <- list()


cat("######## Training seed algorithms naive approach ######## \n")
for(alg in 1:length(seed_algorithms)){
    mean_time.seed[[alg]] = 0
    
    metrics.final.seed <- list()
    metrics.test.seed <- list()
    cat(sprintf("Seed algorithm: %d\n", alg))
    for (i in 1:length(folds)) {
        cat(sprintf("Fold %d out of %d\n", i, length(folds)))
        cat("------------------------------------\n")
        
    
        
        
        train.set <- data.train[unlist(folds[i]),]
        test.set <- data.train[-unlist(folds[i]),]
    
        
        start_time <- Sys.time()
        model_seed <- seed_algorithms[[alg]](train.set, metrics, OUTPUT_VAR,metric_optimize)
        end_time <- Sys.time()
        mean_time.seed[[alg]]= mean_time.seed[[alg]] +
        end_time - start_time
        print(end_time - start_time)
        
        
        metrics.final.seed <- append( metrics.final.seed, metrics_all(metricPreparedFrame(model_seed, test.set)))
        metrics.test.seed <- append( metrics.test.seed, metrics_all(metricPreparedFrame(model_seed, data.test)))
        
        cat("------------------------------------\n")
    }
    
  mean_metrics.seed <- append(mean_metrics.seed, 
        apply(matrix(unlist(metrics.final.seed), ncol= number_metrics, byrow=T), 2, mean))
  
  mean_metrics.test.seed <- append(mean_metrics.test.seed, 
        apply(matrix(unlist(metrics.test.seed), ncol= number_metrics, byrow=T), 2, mean))
}





matrix_mean.seed <- matrix(mean_metrics.seed, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.seed<- cbind(matrix_mean.seed, lapply(mean_time.seed, function(x){as.numeric(x, units="secs")}/length(folds)))
colnames(matrix_mean.seed) <-metric_names
rownames(matrix_mean.seed) <- alg_names


matrix_mean.test.seed <- matrix(mean_metrics.test.seed, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.test.seed <- cbind(matrix_mean.test.seed, lapply(mean_time.seed, function(x){as.numeric(x, units="secs")}/length(folds)))
colnames(matrix_mean.test.seed) <-metric_names
rownames(matrix_mean.test.seed) <- alg_names






cat("SEED ALGORITHM NAIVE \n")
cat("On train\n")
print(matrix_mean.seed)
cat("\n")
cat("On test\n")
print(matrix_mean.test.seed)
cat("\n")

```


Next we will train seed algorithms performing with the direct data but undersampled in the same way our IPIP method will be.

```{r warning=FALSE, cache=TRUE}
mean_metrics.seed_under <- list()
mean_metrics.test.seed_under <- list()
mean_time.seed_under <- list()


train.seeds_under <- list()


for(i in 1:length(folds)){
  
    train.set <- data.train[unlist(folds[i]),]
    minoritary <- subset(train.set, train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
    majoritary<- subset(train.set, train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)
    
    nmin = nrow(minoritary)
    nmaj = nrow(majoritary)
    
    id.majoritary <- sample(x= 1: nmaj, size = nmin/(1/prop.maj -1 )) #Indexes for majoritary class for each subset
    train.seeds_under[[i]] <- rbind(minoritary,majoritary[id.majoritary,])
  }



cat("######## Training seed algorithms undersampled ######## \n")
for(alg in 1:length(seed_algorithms)){
    mean_time.seed_under[[alg]] = 0
    
    cat(sprintf("Seed algorithm: %d\n", alg))
        
    metrics.final.seed_under <- list()
    metrics.test.seed_under <- list()
    for (i in 1:length(folds)) {

        cat(sprintf("Fold %d out of %d\n", i, length(folds)))
        cat("------------------------------------\n")
        

        
        train.set <- train.seeds_under[[i]]
        test.set <- data.train[-unlist(folds[i]),]
    
        
        start_time <- Sys.time()
        model_seed_under <- seed_algorithms[[alg]](train.set, metrics, OUTPUT_VAR, metric_optimize)
        end_time <- Sys.time()
        mean_time.seed_under[[alg]]= mean_time.seed_under[[alg]] +
        end_time - start_time
        print(end_time - start_time)
        
        metrics.final.seed_under <- append( metrics.final.seed_under, metrics_all(metricPreparedFrame(model_seed_under, test.set)))
        metrics.test.seed_under <- append( metrics.test.seed_under, metrics_all(metricPreparedFrame(model_seed_under, data.test)))
        
        cat("------------------------------------\n")
    }
  
  mean_metrics.seed_under <- append(mean_metrics.seed_under, 
        apply(matrix(unlist(metrics.final.seed_under), ncol= number_metrics, byrow=T), 2, mean))
  
    
  mean_metrics.test.seed_under <- append(mean_metrics.test.seed_under, 
        apply(matrix(unlist(metrics.test.seed_under), ncol= number_metrics, byrow=T), 2, mean))
  
  
}





matrix_mean.seed_under <- matrix(mean_metrics.seed_under, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.seed_under<- cbind(matrix_mean.seed_under, lapply(mean_time.seed_under, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.seed_under) <-metric_names
rownames(matrix_mean.seed_under) <- alg_names


matrix_mean.test.seed_under <- matrix(mean_metrics.test.seed_under, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.test.seed_under <- cbind(matrix_mean.test.seed_under, lapply(mean_time.seed_under, function(x){as.numeric(x, units="secs")}/length(folds)))
colnames(matrix_mean.test.seed_under) <-metric_names
rownames(matrix_mean.test.seed_under) <- alg_names


cat("SEED ALGORITHM UNDERSAMPLED \n")
cat("On train\n")
print(matrix_mean.seed_under)
cat("\n")
cat("On test\n")
print(matrix_mean.test.seed_under)
cat("\n")

```


After that approach we will use SMOTE for a balance in the same proportion as we will balance IPIP

```{r warning=FALSE, cache=TRUE}
mean_metrics.SMOTE <- list()
mean_metrics.test.SMOTE <- list()
mean_time.SMOTE <- list()


train.SMOTE <- list()


for(i in 1:length(folds)){
    
    train.set <- data.train[unlist(folds[i]),]
    
    minoritary <- subset(train.set, train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
    majoritary<- subset(train.set, train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)
    
    nmin = nrow(minoritary)
    
    # #We reduce tha maj class to its half
    #nmaj = nrow(majoritary)/2
    #prop.min = 1-prop.maj
    #oversampling_perc = floor(100*(nmaj/((1/prop.min -1)*nmin) -1))
    
    maj_class_perc = 100/(1/prop.maj -1 )
    train.set = SMOTE(as.formula(sprintf("%s ~.", OUTPUT_VAR)) , train.set , k=5, perc.over = 200, perc.under = maj_class_perc ) 
    
    minoritary <- subset(train.set, train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
    majoritary<- subset(train.set, train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)

    train.SMOTE[[i]] <- rbind(minoritary,majoritary)
  }



cat("######## Training seed algorithms SMOTE ######## \n")
for(alg in 1:length(seed_algorithms)){
    mean_time.SMOTE[[alg]] = 0
    
    cat(sprintf("Seed algorithm: %d\n", alg))
    
    metrics.final.SMOTE <- list()
    metrics.test.SMOTE <- list()
    for (i in 1:length(folds)) {

        cat(sprintf("Fold %d out of %d\n", i, length(folds)))
        cat("------------------------------------\n")
        
    

        
        train.set <- train.SMOTE[[i]]
        test.set <- data.train[-unlist(folds[i]),]
    
        
        start_time <- Sys.time()
        model_SMOTE <- seed_algorithms[[alg]](train.set, metrics, OUTPUT_VAR, metric_optimize)
        end_time <- Sys.time()
        mean_time.SMOTE[[alg]]= mean_time.SMOTE[[alg]] +
        end_time - start_time
        print(end_time - start_time)
        
     
        
                
        metrics.final.SMOTE <- append( metrics.final.SMOTE, metrics_all(metricPreparedFrame(model_SMOTE,  test.set)))
        metrics.test.SMOTE <- append( metrics.test.SMOTE, metrics_all(metricPreparedFrame(model_SMOTE, data.test)))
        
        cat("------------------------------------\n")
    }
  
  mean_metrics.SMOTE <- append(mean_metrics.SMOTE, 
      apply(matrix(unlist(metrics.final.SMOTE), ncol= number_metrics, byrow=T), 2, mean))
  
  mean_metrics.test.SMOTE <- append(mean_metrics.test.SMOTE, 
       apply(matrix(unlist(metrics.test.SMOTE), ncol= number_metrics, byrow=T), 2, mean))
}





matrix_mean.SMOTE <- matrix(mean_metrics.SMOTE, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.SMOTE<- cbind(matrix_mean.SMOTE, lapply(mean_time.SMOTE, function(x){as.numeric(x, units="secs")/length(folds)}))


colnames(matrix_mean.SMOTE) <-metric_names
rownames(matrix_mean.SMOTE) <- alg_names


matrix_mean.test.SMOTE <- matrix(mean_metrics.test.SMOTE, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.test.SMOTE <- cbind(matrix_mean.test.SMOTE, lapply(mean_time.SMOTE, function(x){as.numeric(x, units="secs")}/length(folds)))

colnames(matrix_mean.test.SMOTE) <-metric_names
rownames(matrix_mean.test.SMOTE) <- alg_names


cat("SEED ALGORITHM SMOTE\n")
cat("On train\n")
print(matrix_mean.SMOTE)
cat("\n")
cat("On test\n")
print(matrix_mean.test.SMOTE)
cat("\n")

```


Before IPIP models let us set up a simplified function to better work them

```{r}
metricPreparedFrameIPIP <- function(ensemble, setToTest)
      data.frame(
          obs = setToTest[[OUTPUT_VAR]],
          pred = as.factor(prediction.final(ensemble,setToTest[,names(setToTest) != OUTPUT_VAR])),
          prob= prediction.final.prob(ensemble, setToTest[,names(setToTest) != OUTPUT_VAR]),
          obs.prob = as.numeric(ifelse(setToTest[[OUTPUT_VAR]] == OUTPUT_MAJ, 1, 0))
        )

```



Now we will perform the training in the mixed algorithm exhaustive
approach of IPIP

```{r warning=FALSE, cache=TRUE}
mean_metrics.IPIPexhaustMixed <- list()
mean_metrics.test.IPIPexhaustMixed <- list()
mean_time.IPIPexhaustMixed = 0
metrics.final.IPIPexhaustMixed <- list()
metrics.test.IPIPexhaustMixed <- list()


cat("######## Training mixed exhaustive IPIP ######## \n")
 for (i in 1:length(folds)) {
    cat(sprintf("Fold %d out of %d\n",i, length(folds)))
    cat("------------------------------------\n")
      

    
    train.set <- data.train[unlist(folds[i]),]
    test.set <- data.train[-unlist(folds[i]),]
    
    nmin = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
    nmaj = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)
    
    np <- calculate_np( nmin, nmaj)
    p <- calculate_p(np)
    b <- calculate_b(np, nmin, nmaj)
    
    
    start_time <- Sys.time()
    ensemble.fold <- train_IPIP_exhaustive(prop.maj, OUTPUT_VAR, OUTPUT_MIN,
          OUTPUT_MAJ, train.set, test.set, seed_algorithms, prediction,  metrics,np, p, metric_optimize, metric_optimize)
    
    end_time <- Sys.time()
    mean_time.IPIPexhaustMixed = mean_time.IPIPexhaustMixed + end_time - start_time
    
    cat(sprintf("Ensembles length in exhaustive MIXED fold: %d\nLength of ensembles:", i))
    cat(unlist(lapply(ensemble.fold,length)))
    cat("\n")
    
    
    metrics.final.IPIPexhaustMixed <- append( metrics.final.IPIPexhaustMixed, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, test.set)))
    
    metrics.test.IPIPexhaustMixed <- append( metrics.test.IPIPexhaustMixed, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, data.test)))
      cat("------------------------------------\n")

 }


mean_metrics.IPIPexhaustMixed <- append(mean_metrics.IPIPexhaustMixed,
  apply(matrix(unlist(metrics.final.IPIPexhaustMixed), ncol= number_metrics, byrow=T), 2, mean))

matrix_mean.IPIPexhaustMixed <- matrix(mean_metrics.IPIPexhaustMixed, nrow = 1, ncol = number_metrics, byrow = TRUE)
matrix_mean.IPIPexhaustMixed <- cbind(matrix_mean.IPIPexhaustMixed, as.numeric(mean_time.IPIPexhaustMixed, units = "secs")/length(folds))

colnames(matrix_mean.IPIPexhaustMixed) <- metric_names



mean_metrics.test.IPIPexhaustMixed <- append(mean_metrics.test.IPIPexhaustMixed,
  apply(matrix(unlist(metrics.test.IPIPexhaustMixed), ncol= number_metrics, byrow=T), 2, mean))

matrix_mean.test.IPIPexhaustMixed <- matrix(mean_metrics.test.IPIPexhaustMixed, nrow = 1, ncol = number_metrics, byrow = TRUE)
matrix_mean.test.IPIPexhaustMixed <- cbind(matrix_mean.test.IPIPexhaustMixed, as.numeric(mean_time.IPIPexhaustMixed, units = "secs")/length(folds))

colnames(matrix_mean.test.IPIPexhaustMixed) <- metric_names

cat("IPIP EXHAUST MIXED\n")
cat("On train\n")
print(matrix_mean.IPIPexhaustMixed)
cat("\n")
cat("On test\n")
print(matrix_mean.test.IPIPexhaustMixed)
cat("\n")

```

We then try the exhaustive method applied to a group of algorithms of
the same kind

```{r warning=FALSE, cache=TRUE}
mean_metrics.IPIPexhaustRepeat <- list()
mean_metrics.test.IPIPexhaustRepeat <- list()
mean_time.IPIPexhaustRepeat <- list()

cat("######## Training exhaustive repeat IPIP ######## \n")
for(alg in 1:length(seed_algorithms)){
  
   cat(sprintf("Seed algorithm: %d\n", alg))
  
   mean_time.IPIPexhaustRepeat[[alg]] = 0
   metrics.final.IPIPexhaustRepeat <- list()
   metrics.test.IPIPexhaustRepeat <- list()
   
   for (i in 1:length(folds)) {
      cat(sprintf("Fold %d out of %d\n",i, length(folds)))
      cat("------------------------------------\n")
        
      train.set <- data.train[unlist(folds[i]),]
      test.set <- data.train[-unlist(folds[i]),]
      
      nmin = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
      nmaj = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)
      
      np <- calculate_np( nmin, nmaj)
      p <- calculate_p(np)
      b <- calculate_b(np, nmin, nmaj)
      
      conf<- get_function_vector(b, seed_algorithms[alg])
      
      start_time <- Sys.time()
      
      ensemble.fold <- train_IPIP_exhaustive(prop.maj, OUTPUT_VAR, OUTPUT_MIN, OUTPUT_MAJ, train.set, test.set, conf, prediction,  metrics,np, p, metric_optimize, metric_optimize)
      
      end_time <- Sys.time()
      mean_time.IPIPexhaustRepeat[[alg]]= mean_time.IPIPexhaustRepeat[[alg]] + end_time - start_time
      
      cat(sprintf("Ensembles length in exhaustive REPEAT fold: %d\nLength of ensembles:", i))
      cat(unlist(lapply(ensemble.fold,length)))
      cat("\n")
    
  
      metrics.final.IPIPexhaustRepeat <- append( metrics.final.IPIPexhaustRepeat, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, test.set)))
    
      metrics.test.IPIPexhaustRepeat <- append( metrics.test.IPIPexhaustRepeat, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, data.test)))
          cat("------------------------------------\n")

   }
    mean_metrics.IPIPexhaustRepeat <- append(mean_metrics.IPIPexhaustRepeat, 
    apply(matrix(unlist(metrics.final.IPIPexhaustRepeat), ncol= number_metrics, byrow=T), 2, mean))
   
    mean_metrics.test.IPIPexhaustRepeat <- append(mean_metrics.test.IPIPexhaustRepeat, 
    apply(matrix(unlist(metrics.test.IPIPexhaustRepeat), ncol= number_metrics, byrow=T), 2, mean))
}

matrix_mean.IPIPexhaustRepeat <- matrix(mean_metrics.IPIPexhaustRepeat, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.IPIPexhaustRepeat <- cbind(matrix_mean.IPIPexhaustRepeat, lapply(mean_time.IPIPexhaustRepeat, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.IPIPexhaustRepeat) <- metric_names
rownames(matrix_mean.IPIPexhaustRepeat) <- alg_names


matrix_mean.test.IPIPexhaustRepeat <- matrix(mean_metrics.test.IPIPexhaustRepeat, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.test.IPIPexhaustRepeat <- cbind(matrix_mean.test.IPIPexhaustRepeat, lapply(mean_time.IPIPexhaustRepeat, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.test.IPIPexhaustRepeat) <- metric_names
rownames(matrix_mean.test.IPIPexhaustRepeat) <- alg_names


cat("IPIP EXHAUST REPEATED\n")
cat("On validation\n")
print(matrix_mean.IPIPexhaustRepeat)
cat("\n")
cat("On test\n")
print(matrix_mean.test.IPIPexhaustRepeat)
cat("\n")

```

Let us finally execute the code on the sequential greedy original
approach of IPIP

```{r warning=FALSE, cache=TRUE}

mean_metrics.IPIPrepeated <- list()
mean_metrics.test.IPIPrepeated <- list()
mean_time.IPIPrepeated <- list()

cat("######## Training sequential repeat IPIP ######## \n")
for(alg in 1:length(seed_algorithms)){
  
   cat(sprintf("Seed algorithm: %d\n", alg))
   mean_time.IPIPrepeated[[alg]] = 0
   metrics.final.IPIPrepeated <- list()
   metrics.test.IPIPrepeated <- list()
  
   for (i in 1:length(folds)) {
      cat(sprintf("Fold %d out of %d\n",i, length(folds)))
      cat("------------------------------------\n")
  
      train.set <- data.train[unlist(folds[i]),]
      test.set <- data.train[-unlist(folds[i]),]
      
      nmin = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
      nmaj = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)
      
      np <- calculate_np( nmin, nmaj)
      p <- calculate_p(np)
      b <- calculate_b(np, nmin, nmaj)
    
      conf<- get_function_vector(b, seed_algorithms[alg])
      
      start_time <- Sys.time()
      ensemble.fold <- train_IPIP(prop.maj, OUTPUT_VAR, OUTPUT_MIN, OUTPUT_MAJ, train.set, test.set, conf, prediction,  metrics, b, np, p, mt, metric_optimize, metric_optimize)
      end_time <- Sys.time()
      mean_time.IPIPrepeated[[alg]]= mean_time.IPIPrepeated[[alg]] +
              end_time - start_time
      
      
      cat(sprintf("Ensembles length in sequential in fold: %d\nLength of ensembles:", i))
      cat(unlist(lapply(ensemble.fold,length)))
      cat("\n")
      
      metrics.final.IPIPrepeated <- append( metrics.final.IPIPrepeated, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, test.set)))
    
      metrics.test.IPIPrepeated <- append( metrics.test.IPIPrepeated, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, data.test)))
        cat("------------------------------------\n")

  }

  mean_metrics.IPIPrepeated <- append(mean_metrics.IPIPrepeated, apply(matrix(unlist(metrics.final.IPIPrepeated), ncol= number_metrics, byrow=T), 2, mean))
  mean_metrics.test.IPIPrepeated <- append(mean_metrics.test.IPIPrepeated, apply(matrix(unlist(metrics.test.IPIPrepeated), ncol= number_metrics, byrow=T), 2, mean))

}


matrix_mean.IPIPrepeated <- matrix(mean_metrics.IPIPrepeated, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.IPIPrepeated <- cbind(matrix_mean.IPIPrepeated, lapply(mean_time.IPIPrepeated, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.IPIPrepeated) <- metric_names
rownames(matrix_mean.IPIPrepeated) <- alg_names

matrix_mean.test.IPIPrepeated <- matrix(mean_metrics.test.IPIPrepeated, nrow = length(seed_algorithms), ncol = number_metrics, byrow = TRUE)
matrix_mean.test.IPIPrepeated <- cbind(matrix_mean.test.IPIPrepeated, lapply(mean_time.IPIPrepeated, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.test.IPIPrepeated) <- metric_names
rownames(matrix_mean.test.IPIPrepeated) <- alg_names


cat("IPIP SEQUENTIAL REPEAT\n")
cat("On train:\n")
print(matrix_mean.IPIPrepeated)
cat("On test:\n")
print(matrix_mean.test.IPIPrepeated)
cat("\n")


```

And, last, we will execute the sequential approach but to a mixed version of algorithms

```{r, warning=FALSE, cache = TRUE}
mean_metrics.IPIPseqMixed<- list()
mean_metrics.test.IPIPseqMixed<- list()
mean_time.IPIPseqMixed = 0 
   
conf<- c(seed_algorithms, seed_algorithms)
cat("######## Training sequential mixed IPIP ######## \n")


   metrics.final.IPIPseqMixed <- list()
   metrics.test.IPIPseqMixed <- list()
  
   for (i in 1:length(folds)) {
      cat(sprintf("Fold %d out of %d\n",i, length(folds)))
      cat("------------------------------------\n")
  
      train.set <- data.train[unlist(folds[i]),]
      test.set <- data.train[-unlist(folds[i]),]
      
      nmin = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MIN)
      nmaj = sum(train.set[[OUTPUT_VAR]] == OUTPUT_MAJ)
      
      np <- calculate_np( nmin, nmaj)
      p <- calculate_p(np)
      b <- calculate_b(np, nmin, nmaj)
    

      
      start_time <- Sys.time()
      ensemble.fold <- train_IPIP(prop.maj, OUTPUT_VAR, OUTPUT_MIN, OUTPUT_MAJ, train.set, test.set, conf, prediction,  metrics, b, np, p, mt, metric_optimize, metric_optimize)
      end_time <- Sys.time()
      mean_time.IPIPseqMixed= mean_time.IPIPseqMixed +
              end_time - start_time
      
      
      cat(sprintf("Ensembles length in sequential in fold: %d\nLength of ensembles:", i))
      cat(unlist(lapply(ensemble.fold,length)))
      cat("\n")
    
      
      metrics.final.IPIPseqMixed <- append( metrics.final.IPIPseqMixed, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, test.set)))
    
      metrics.test.IPIPseqMixed <- append( metrics.test.IPIPseqMixed, 
      metrics_all(metricPreparedFrameIPIP(ensemble.fold, data.test)))
        cat("------------------------------------\n")




}

mean_metrics.IPIPseqMixed <- append(mean_metrics.IPIPseqMixed, apply(matrix(unlist(metrics.final.IPIPseqMixed), ncol= number_metrics, byrow=T), 2, mean))
matrix_mean.IPIPseqMixed <- matrix(mean_metrics.IPIPseqMixed, nrow = 1, ncol = number_metrics, byrow = TRUE)
matrix_mean.IPIPseqMixed <- cbind(matrix_mean.IPIPseqMixed,lapply(mean_time.IPIPseqMixed, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.IPIPseqMixed) <- metric_names

mean_metrics.test.IPIPseqMixed <- append(mean_metrics.test.IPIPseqMixed, apply(matrix(unlist(metrics.test.IPIPseqMixed), ncol= number_metrics, byrow=T), 2, mean))
matrix_mean.test.IPIPseqMixed <- matrix(mean_metrics.test.IPIPseqMixed, nrow = 1, ncol = number_metrics, byrow = TRUE)
matrix_mean.test.IPIPseqMixed <- cbind(matrix_mean.test.IPIPseqMixed,lapply(mean_time.IPIPseqMixed, function(x){as.numeric(x, units="secs")/length(folds)}))
colnames(matrix_mean.test.IPIPseqMixed) <- metric_names


cat("IPIP SEQUENTIAL MIXED\n")
cat("On train:\n")
print(matrix_mean.IPIPseqMixed)
cat("On test:\n")
print(matrix_mean.test.IPIPseqMixed)
cat("\n")
```

After executing the code, we will show the comparison of relevant
metrics using this function 

```{r, results= 'hold'}

print_with_asterisks <- function(matrix, best_values, elem) {
    num_spaces = 11
    for(j in 0:nrow(matrix)){
        for (i in 0:ncol(matrix)) {
          if(j==0 && i==0){
            str_to_cat = " "
          }
          else if(i==0){
            str_to_cat = alg_names[j]
          }
          else if(j == 0){
            str_to_cat = metric_names[i]          
          }
          else{
              if(is.nan(matrix[[j,i]]) == FALSE && matrix[[j, i]] == best_values[[i,elem]]) {
                if ((i!=13 && best_values[[i,elem]] == max(unlist(best_values[i,]))) || (i==13&&best_values[[i,elem]]==min(unlist(best_values[i,])))){
                  str_to_cat = sprintf("%f**", matrix[[j,i]])
                } else {
                  str_to_cat = sprintf("%f*", matrix[[j,i]])
                }
              }
              else{
                str_to_cat = sprintf("%f", matrix[[j,i]])
              }
              
              
              
          }
          cat(str_to_cat)
          cat(strrep(" ", num_spaces - nchar(str_to_cat)))
      }
      cat("\n")
    }
    cat("\n")
}

save(matrix_mean.IPIPrepeated, matrix_mean.IPIPseqMixed, matrix_mean.IPIPexhaustRepeat,
     matrix_mean.IPIPexhaustMixed, matrix_mean.seed, matrix_mean.seed_under, matrix_mean.SMOTE,matrix_mean.test.IPIPrepeated, matrix_mean.test.IPIPseqMixed, matrix_mean.test.IPIPexhaustRepeat,matrix_mean.test.IPIPexhaustMixed, matrix_mean.test.seed, matrix_mean.test.seed_under, matrix_mean.test.SMOTE,
     file = "./saved_matrices.RData")

get_better_data <- function(matrix){
  best_data <- list()
  for (col in metric_names){
    if(col == "TIME"){
      best_data <- append(best_data ,min(unlist(matrix[,col]), na.rm=TRUE))
    }
    else{
      best_data <- append(best_data, max(unlist(matrix[,col]), na.rm=TRUE))
    }
  }
  return(best_data)
}
```


Now we get the max values and print the asterisks if it is the max of its matrix and double ** if it is the max of all matrix in that category
```{r}
cat("----------------------------------------------\n")
cat("Comparing on train ALL\n")

best_values <- sapply( list(
  matrix_mean.IPIPrepeated,
  matrix_mean.IPIPseqMixed,
  matrix_mean.IPIPexhaustRepeat,
  matrix_mean.IPIPexhaustMixed,
  matrix_mean.seed,
  matrix_mean.seed_under,
  matrix_mean.SMOTE
), get_better_data)

cat("IPIP SEQUENTIAL REPEAT\n")
print_with_asterisks(matrix_mean.IPIPrepeated,best_values,1)

cat("IPIP SEQUENTIAL MIXED\n")
print_with_asterisks(matrix_mean.IPIPseqMixed,best_values,2)

cat("IPIP EXHAUST REPEAT\n")
print_with_asterisks(matrix_mean.IPIPexhaustRepeat, best_values, 3)

cat("IPIP EXHAUST MIXED\n")
print_with_asterisks(matrix_mean.IPIPexhaustMixed, best_values, 4)

cat("SEED\n")
print_with_asterisks(matrix_mean.seed, best_values, 5)

cat("SEED UNDERSAMPLED\n")
print_with_asterisks(matrix_mean.seed_under, best_values, 6)

cat("SMOTE\n")
print_with_asterisks(matrix_mean.SMOTE, best_values, 7)
```

And comparing into the small subset of test

```{r}
cat("----------------------------------------------\n")
cat("Comparing on test ALL\n")

best_values <- sapply( list(
  matrix_mean.test.IPIPrepeated,
  matrix_mean.test.IPIPseqMixed,
  matrix_mean.test.IPIPexhaustRepeat,
  matrix_mean.test.IPIPexhaustMixed,
  matrix_mean.test.seed,
  matrix_mean.test.seed_under,
  matrix_mean.test.SMOTE
), get_better_data)

cat("IPIP SEQUENTIAL REPEAT\n")
print_with_asterisks(matrix_mean.test.IPIPrepeated,best_values,1)

cat("IPIP SEQUENTIAL MIXED\n")
print_with_asterisks(matrix_mean.test.IPIPseqMixed,best_values,2)

cat("IPIP EXHAUST REPEAT\n")
print_with_asterisks(matrix_mean.test.IPIPexhaustRepeat, best_values, 3)

cat("IPIP EXHAUST MIXED\n")
print_with_asterisks(matrix_mean.test.IPIPexhaustMixed, best_values, 4)

cat("SEED\n")
print_with_asterisks(matrix_mean.test.seed, best_values, 5)

cat("SEED UNDERSAMPLED\n")
print_with_asterisks(matrix_mean.test.seed_under, best_values, 6)

cat("SMOTE\n")
print_with_asterisks(matrix_mean.test.SMOTE, best_values, 7)
```

And now only insights comparing different IPIP approaches

```{r}
cat("----------------------------------------------\n")
cat("Comparing on train ONLY IPIP\n")

best_values <- sapply( list(
  matrix_mean.IPIPrepeated,
  matrix_mean.IPIPseqMixed,
  matrix_mean.IPIPexhaustRepeat,
  matrix_mean.IPIPexhaustMixed
), get_better_data)

cat("IPIP SEQUENTIAL REPEAT\n")
print_with_asterisks(matrix_mean.IPIPrepeated,best_values,1)

cat("IPIP SEQUENTIAL MIXED\n")
print_with_asterisks(matrix_mean.IPIPseqMixed,best_values,2)

cat("IPIP EXHAUST REPEAT\n")
print_with_asterisks(matrix_mean.IPIPexhaustRepeat, best_values, 3)

cat("IPIP EXHAUST MIXED\n")
print_with_asterisks(matrix_mean.IPIPexhaustMixed, best_values, 4)

```

```{r}
cat("----------------------------------------------\n")
cat("Comparing on test ONLY IPIP\n")

best_values <- sapply( list(
  matrix_mean.test.IPIPrepeated,
  matrix_mean.test.IPIPseqMixed,
  matrix_mean.test.IPIPexhaustRepeat,
  matrix_mean.test.IPIPexhaustMixed
), get_better_data)

cat("IPIP SEQUENTIAL REPEAT\n")
print_with_asterisks(matrix_mean.test.IPIPrepeated,best_values,1)

cat("IPIP SEQUENTIAL MIXED\n")
print_with_asterisks(matrix_mean.test.IPIPseqMixed,best_values,2)

cat("IPIP EXHAUST REPEAT\n")
print_with_asterisks(matrix_mean.test.IPIPexhaustRepeat, best_values, 3)

cat("IPIP EXHAUST MIXED\n")
print_with_asterisks(matrix_mean.test.IPIPexhaustMixed, best_values, 4)
```
